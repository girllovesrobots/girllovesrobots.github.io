<!doctype html>
<html>
<head>
  <!--Google Font reference-->
    <!--Roboto Slab: Normal; Open Sans: Light Italic, Normal-->
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400|Roboto+Slab' rel='stylesheet' type='text/css'>
  <title>Projects</title>
  <link rel="stylesheet" type= "text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel = "stylesheet" type= "text/css" href="main.css">
  <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
</head>
<body>
<header>
<!-- Navigation -->
<nav class = "navigation">
  <div class = "home">
    <a href= "index.html">HOME</a>
  </div>
  <div class = "about">
    <a href= "about.html">ABOUT</a>
  </div>
	<div class = "projects">
    <a href= "projects.html">PROJECTS</a>
  </div>
</nav>
</header>

<!--Main Content-->
<h1>Subit</h1>
<p class="centered-image"><img src="images/subit_logo.png" height ="210" width ="210"/></p>
<p>June 19th - June 20th</p>
<p>Approximately 1,000,000 people in the United States over the age of 5 are functionally deaf. Imagine having an application that could augment their reality, giving them the ability to attend talks and tours without a supplemental guide book. This assistive application for the SamsungVR, Subit, provides realtime captioning for your life. Upon running the application, the wearer can simply tap the touch pad to start the voice recognition process. Following that, captions will appear beneath the camera feed viewing area. This application uses the Android Native SDK, Google Voice Recognition and Oculus Mobile for the Samsung GearVR. Click <a href="https://github.com/girllovesrobots/sffc"><b>here</b></a> to see our open source code.</p>


<br />
<br />
<br />

<h1>SomeWear</h1>
<p class="centered-image"><img src="images/first.png" height="210" width="162"/><img src="images/second.png" height="210" width="162"/><img src="images/third.png" height="210" width="162"/><img src="images/fourth.png" height="210" width="162"/><img src="images/sixth.png" height="210" width="162"/></p>
<p>Feb 2015 - May 2015</p>
<p>SomeWear is a revolutionary smart watch application that capitalizes on a user's need for instant gratification. It aims to provide real time, specific destination for a user catered to their preferences. Using several Google APIs, SomeWear quickly finds interesting places to go and automatically plans out a walkable route from the user's location. This application ensures that users are never directionless, always having "somewear" to go.</p>
<p class="centered-image"><iframe width="560" height="315" src="https://www.youtube.com/embed/cDEWZfm09bo" frameborder="0" allowfullscreen></iframe></p>

<br />
<br />
<br />

<h1>"Kinect"ing with Robots</h1>
<p class="centered-image"><img src="images/aida.jpg" height="480" width="270"/></p>
<p>Jan 2015 - </p>
<p>To better integrate intelligent machines in our daily lives, social awareness must be emphasized when developing behavioral algorithms for robots. My work involved creating a multifunction Windows Presentation Foundation (WPF) application that collects and sends sensory data from the Kinect 2.0 sensor to a robot. I also used Robot Operating System (ROS), to parse sensory inputs and coordinate data flow between the WPF application tied to the Kinect sensor and AIDA, a robot used as a testbed for the demo. I also designed and implemented behavioral algorithms for an interactive human-robot demo using the Kinect 2.0 and AIDA for sponsors of the MIT Media Lab.</p>
<p class="centered-image"><img src="images/kinect.png" height="580" width="840"/></p>

<br />
<br />
<br />

<h1>NASA RASCAL RoboOps</h1>
<p class="centered-image"><img src="images/rover.png" height="320" width="460"/></p>
<p>Jan 2014 - May 2014</p>
<p>NASA's RASC-AL RoboOps is an engineering competition sponsored by NASA and organized by the National Institute of Aerospace where teams of undergraduate and graduate students are invited to create a multi-disciplinary team to build a planetary rover prototype and demonstrate its capabilities at the NASA Johnson Space Center. All participating teams are required to design and build a physical rover, submit a technical report, and do a live demonstration of the rover's functionalities.</p>
<p class="centered-image"><img style="padding:10px" src="images/boa_filter.png" height="256" width="368"/><img style="padding:10px" src="images/boa.png" height="256" width="368"/></p>
<p>As Vision System Lead in 2014, I developed image processing algorithms using Teledyne Dalsa's Sherlock program for the BOA PRO camera to identify colored rocks in a mock planetary environment at NASA Johnson Space Center. Images were processed using a combination of filters and blob detection techniques to identify potential areas of interest. In the scope of this competition, various colored rocks and plastic toy "martians" were the target objects the robot needed to collect. The team placed second in the competition. A copy of our final technical report can be found below.</p>
<h3><li><a href="roboteam_report.pdf">2014 MIT Robotics Team Final Technical Report</a></li></h3>
<br />
<br />
<br />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68526909-1', 'auto');
  ga('send', 'pageview');

</script>
</body>

<!--Footer-->
<footer>
</footer>
</html>
